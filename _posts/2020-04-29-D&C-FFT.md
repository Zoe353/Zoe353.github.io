---
layout: post
title: "Zhimin Sun, Divide and Conquer-FFT"
markdown: kramdown
date: 2020-04-29
---

Polynomial multiplication  
Generally:  
$A(x) = a_0 + a_1 x + ... + a_d x^d$
$B(x) = b_0 + b_1 x + ... + b_d x^d$  
$C(x) = A(x)B(x) = c_0 + c_1 x + ... + c_{2d} x^{2d}$, 
which has coefficients $c_k = a_0 b_k + a_1 b_{k-1} + ... + a_k b_0$  
Compute $c_k$ from the formula above takes $O(k)$ steps, and finding all $2d+1$ coeffients
would therefore seem to require $\Theta (d^2)$ time.  
To do better, we use FFT(the fast fourier transform).  
Before we dive into FFT, we firstly look into a fact.  
Fact: A degree-d polynomial is uniquely characterized by its value at any $d+1$ distinct 
points.  
Thus, we could specify a degree-d polynomial $A(x) = a_0 + a_1 x + a_d x^d$ by either one 
of the following:  
1. Its coefficients $a_0, a_1, ..., a_d$
2. The values $A(x_0), A(x_1), ...,A(x_d)$, (fix any distinct points $x_0, x_1, ..., x_d$).  

With that, we could transform the initial process of polynomial multiplication into the steps listed before.  
Input: Coefficients of two polynomials, $A(x) and B(x)$ of degree $d$.  
Output: Their product $C = AB$  
Selection: pick some points $x_0, x_1, ...,x_{n-1}$, where n $\geq 2d+1$.  -- just linear time  
Evaluation: compute $A(x_0), A(x_1), ..., A(x_{n-1})$ and $B(x_0), B(x_1), ..., B(x_{n-1})$  
Multiplication: compute $C(x_k) = A(x_k)B(x_k)$ for $k=0,..,n-1$  --n multiplications, just linear time  
Interpolation: Recover $C(x) = c_0 + c_1 x + ...+c_{2d} x^{2d}$  
For the Evaluation step: evaluating a polynomial of degree $d \leq n$ at a single point takes $O(n)$ steps, and so the 
baseline for n points is $\Theta (n^2)$, which is bad.  
So our next step is to solve the evaluation step using the D&C idea.
  
<em> Evaluation by divide and Conquer</em>  
An idea for how to pick the $n$ points at which to evaluate a polynomial $A(x)$ of degree $\leq n-1$: positive-negative pairs.
It could save some computations because of the same parts of the polynomial.(you can write an example to verify that. Eg: A(x)= 3 + x + 3x^2, and plug x_0 , -x_0 int it.)  
The original problem of size $n$ is in this way recast as two subproblems of size $n/2$, followed by some linear-time arithmetic.
If we could recurse, we could get a divide and conquer with running time $T(n) = 2T(n/2) + O(n), which is O(nlogn)$, exactly what we want.
One more issueï¼› To recurse at the next level, we need the $n/2$ evaluation points $x_0^2, x_1^2, ..., x_{n/2}^2$ to be themselves plus-minus pairs.
But how can a square be negative? $\rightarrow$ Use complex numbers.  
It turns out to be: the complex nth roots of unity, that is , the $n$ complex solutions to the equation $Z^n = 1$.
If n is even, the $n^{th}$ roots are plus-minus paires $w^{n/2+j} = -w^j$, squaring them produces the $(n/2)nd$ roots of unity.  

<em>Interpolation - the last remaining piece of the puzzle is the inverse operation</em>  
To get a cleaner view of interpolation, let's explicitly set down the relationship between our two representations for a polynomial A(x)
of degree $\leq n-1$.  
$$\begin{bmatrix} 
A(x_0)\\  
A(x_1)\\
.\\
.\\
A(x_{n-1})
\end{bmatrix}
$$   =    $$\begin{bmatrix}
1 & x_0 & x_0^2 & ... & x_0^{n-1}\\
1 & x_1 & x_1^2 & ... & x_1^{n-1}\\
.\\
.\\
1 & x_{n-1} & x_{n-1}^2 & ... & x_{n-1}^{n-1}\\
\end{bmatrix}
$$   $$\begin{bmatrix}
a_0\\
a_1\\
.\\
.\\
a_{n-1}
\end{bmatrix}
$$  
The second matrix is $M$. Evaluation is multiplication by $M$, while interpolation is multiplication by $M^{-1}$.  
Lemma: The columns of matrix $M$ are orthogonal to each other.  
Inversion formula: $M_n(w)^{-1} = \frac{1}{n} M_n(w^{-1})$  
It turns out, amazingly, that <coefficients> = $\frac{1}{n}$ FFT(<values>,$w^{-1}$).
            





